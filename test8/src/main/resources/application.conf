# 数据仓库配置
datawarehouse {
  # Spark配置
  spark {
    app-name = "DataWarehouse-Pipeline"
    master = "yarn"
    deploy-mode = "cluster"

    # 执行器配置
    executor {
      instances = 10
      cores = 4
      memory = "4g"
      memory-fraction = 0.8
    }

    # 驱动配置
    driver {
      cores = 2
      memory = "2g"
    }

    # 其他配置
    sql {
      adaptive.enabled = true
      adaptive.coalescePartitions.enabled = true
      adaptive.skewJoin.enabled = true
    }

    serializer = "org.apache.spark.serializer.KryoSerializer"
    kryo.registrationRequired = false
  }

  # 数据源配置
  datasource {
    # Hive配置
    hive {
      metastore.uris = "thrift://hadoop-master:9083"
      exec.dynamic.partition = true
      exec.dynamic.partition.mode = "nonstrict"
      exec.max.dynamic.partitions = 2000
    }

    # HDFS配置
    hdfs {
      namenode = "hdfs://hadoop-cluster"
      base-path = "/data/warehouse"
    }
  }

  # 数据库配置
  databases {
    ods = "ods_db"
    dwd = "dwd_db"
    dws = "dws_db"
    ads = "ads_db"
  }

  # 分区配置
  partition {
    date-format = "yyyy-MM-dd"
    retention-days = 90
  }

  # 任务配置
  jobs {
    # ODS到DWD清洗任务
    ods-to-dwd {
      enabled = true
      batch-size = 100000
      checkpoint-interval = "10 minutes"
    }

    # DWD到DWS聚合任务
    dwd-to-dws {
      enabled = true
      window-duration = "1 hour"
      watermark-delay = "10 minutes"
    }
  }

  # 质量检查配置
  quality {
    enabled = true
    null-check-threshold = 0.05
    duplicate-check-enabled = true
  }
}

# 日志配置
logging {
  level = "INFO"
  pattern = "%d{yyyy-MM-dd HH:mm:ss} %-5level %logger{36} - %msg%n"
}