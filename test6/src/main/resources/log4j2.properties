# src/main/resources/log4j2.properties
# Log4j2 configuration. This will be included in the JAR.
# When running on YARN, logs typically go to container logs.

status = warn # Log4j2 internal logging level
name = SparkDWETLPropertiesConfig

# Define appenders: console and potentially a file appender (though YARN handles aggregation)
appenders = console #, rollingFile

# Console Appender Configuration
appender.console.type = Console
appender.console.name = STDOUT
appender.console.layout.type = PatternLayout
# %d{DEFAULT} %highlight{[%-5p]}{FATAL=red, ERROR=red, WARN=yellow, INFO=green, DEBUG=cyan, TRACE=blue} [%t] %C{1.}.%M(%L) - %m%n"
appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36}.%M(%L) - %msg%n%throwable

# Optional: Rolling File Appender (useful for local dev, YARN manages cluster logs)
# appender.rollingFile.type = RollingFile
# appender.rollingFile.name = RollingFileAppender
# appender.rollingFile.fileName = logs/spark-dw-etl.log
# appender.rollingFile.filePattern = logs/spark-dw-etl-%d{yyyy-MM-dd}-%i.log.gz
# appender.rollingFile.layout.type = PatternLayout
# appender.rollingFile.layout.pattern = %d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n%throwable
# appender.rollingFile.policies.type = Policies
# appender.rollingFile.policies.time.type = TimeBasedTriggeringPolicy
# appender.rollingFile.policies.time.interval = 1 # Rollover daily
# appender.rollingFile.policies.time.modulate = true
# appender.rollingFile.policies.size.type = SizeBasedTriggeringPolicy
# appender.rollingFile.policies.size.size = 100MB # Rollover at 100MB
# appender.rollingFile.strategy.type = DefaultRolloverStrategy
# appender.rollingFile.strategy.max = 10 # Keep 10 backup files

# Root Logger Configuration
# Set level to INFO or WARN for production, DEBUG for development.
# AppConfig.sparkLogLevel will set Spark's internal logging, this is for your app's Log4j2.
rootLogger.level = INFO
rootLogger.appenderRef.stdout.ref = STDOUT
# rootLogger.appenderRef.rolling.ref = RollingFileAppender

# Configure logging levels for specific packages
# Your application's base package
logger.com.yourcompany.dw.name = com.yourcompany.dw
logger.com.yourcompany.dw.level = DEBUG # More verbose for your app code during dev/debug

# Spark and related libraries (can be noisy, set to WARN or ERROR in prod)
logger.org.apache.spark.name = org.apache.spark
logger.org.apache.spark.level = WARN

logger.org.eclipse.jetty.name = org.eclipse.jetty
logger.org.eclipse.jetty.level = INFO

logger.org.apache.hadoop.name = org.apache.hadoop
logger.org.apache.hadoop.level = WARN

# For Hive metastore client, if used
logger.org.apache.hadoop.hive.metastore.name = org.apache.hadoop.hive.metastore
logger.org.apache.hadoop.hive.metastore.level = WARN

# DataStax Cassandra connector, if used
# logger.com.datastax.spark.connector.name = com.datastax.spark.connector
# logger.com.datastax.spark.connector.level = WARN