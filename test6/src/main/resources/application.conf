# src/main/resources/application.conf (HOCON format)
# This file will be automatically loaded if it's on the classpath.

spark {
  appName = "SparkDataWarehouseETL"
  master = "local[*]" # Default for local development; YARN will override this when submitting.
  logLevel = "WARN"

  # Example Spark configurations (add more as needed)
  # "spark.sql.shuffle.partitions" = 200
  # "spark.serializer" = "org.apache.spark.serializer.KryoSerializer"
  # "spark.sql.extensions" = "io.delta.sql.DeltaSparkSessionExtension" # If using Delta Lake
  # "spark.sql.catalog.spark_catalog" = "org.apache.spark.sql.delta.catalog.DeltaCatalog" # If using Delta Lake
}

paths {
  ods {
    users = "data/ods/users" # Example: "hdfs:#/user/your_user/dw/ods/users"
    orders = "data/ods/orders" # Example: "s3a:#your-bucket/dw/ods/orders"
  }
  dwd {
    cleaned_users = "data/dwd/cleaned_users" # Example: "hdfs:#/user/your_user/dw/dwd/cleaned_users"
    fact_orders = "data/dwd/fact_orders"
  }
  dws {
    user_summary_daily = "data/dws/user_summary_daily" # Example: "hdfs:#/user/your_user/dw/dws/user_summary_daily"
    product_sales_agg = "data/dws/product_sales_agg"
  }
  sql_scripts_base = "sql" # Base path for SQL scripts within resources
}

etl {
  defaultLoadDate = "1970-01-01" # A fallback default if no date is provided
  outputFormat = "parquet" # e.g., "parquet", "delta", "orc"
  dwdOutputMode = "overwrite" # e.g., "overwrite", "append" for DWD layer
  dwsOutputMode = "overwrite" # e.g., "overwrite", "append" for DWS layer
  # For Delta Lake, you might use merge operations instead of simple overwrite/append.
}